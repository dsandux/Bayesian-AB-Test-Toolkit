{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa75c0ae6c4ba362",
   "metadata": {},
   "source": [
    "---\n",
    "### Hi! My name is Lucas Pereira, and I'm the creator of this notebook. It's great to have you here!\n",
    "\n",
    "You can find the original project on my GitHub and connect with me on LinkedIn.\n",
    "\n",
    "* **Original notebook:** [https://github.com/dsandux/AB-Test-Toolkit](https://github.com/dsandux/AB-Test-Toolkit)\n",
    "* **LinkedIn:** [https://www.linkedin.com/in/lucaspereira](https://www.linkedin.com/in/lucaspereira)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4300b80bcbe16",
   "metadata": {},
   "source": [
    "# A/B Test Analysis with Bayesian Statistics - Which options is the best?\n",
    "This notebook performs an end-to-end analysis of A/B test results using Bayesian statistical methods. The objective is to go beyond the traditional \"p-value\" to calculate the probability of each variant being the best and to quantify the expected risk associated with choosing one variant over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93851f08a4e7811b",
   "metadata": {},
   "source": [
    "#### 1. Setup\n",
    "\n",
    "Here is where we install all the libraries needed to run this test.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a427fe66ce8fc60d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "# --- Install required packages ---\n",
    "!pip install -q pandas numpy scipy matplotlib seaborn ipywidgets tabulate\n",
    "!pip install -q google-generativeai python-pptx\n",
    "!pip install langchain faiss-cpu sentence-transformers tiktoken\n",
    "!pip install sentence-transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "\n",
    "# --- Import Libraries for Data Analysis & Visualization ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# --- Import Libraries for Interactive Widgets & Display ---\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown, FileLink\n",
    "import os\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Import New Libraries for GenAI and Presentations ---\n",
    "import google.generativeai as genai\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "\n",
    "print(\"✅ Libraries loaded successfully. You can now proceed to the Control Panel.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7a627bb742ef4064",
   "metadata": {},
   "source": [
    "#### 2. Upload files and set parameters\n",
    "\n",
    "Run the cell below to see the upload file and settings form. The default settings are better for most of cases (unless you have a specific need and knows what the options means). You can just upload your Excel file and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "id": "5a1de2caa799e5b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "import io\n",
    "\n",
    "# --- Define Layouts for a cleaner and more professional look ---\n",
    "# This helps align widgets and give them consistent sizing and spacing.\n",
    "wide_layout = widgets.Layout(width='98%', margin='5px 0')\n",
    "button_layout = widgets.Layout(width='250px', margin='15px 0 0 0')\n",
    "\n",
    "# --- Create Interactive Widgets ---\n",
    "\n",
    "# File Uploader Widget\n",
    "uploader = widgets.FileUpload(\n",
    "    accept='.xlsx',\n",
    "    description='Upload Data File',\n",
    "    button_style='info',\n",
    "    tooltip='Click to upload your A/B test data in .xlsx format',\n",
    "    layout=wide_layout\n",
    ")\n",
    "\n",
    "# Bayesian Prior Selector Widget\n",
    "prior_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Jeffreys (Recommended for most tests)', (0.5, 0.5)),\n",
    "        ('Uniform (Neutral / Uninformative)', (1.0, 1.0))\n",
    "    ],\n",
    "    value=(0.5, 0.5),\n",
    "    description='Bayesian Prior:',\n",
    "    style={'description_width': 'initial'},\n",
    "    tooltip='Choose the starting assumption for the model.',\n",
    "    layout=wide_layout\n",
    ")\n",
    "\n",
    "# Risk Threshold Slider Widget\n",
    "risk_slider = widgets.FloatSlider(\n",
    "    value=0.01,\n",
    "    min=0.005,\n",
    "    max=0.05,\n",
    "    step=0.005,\n",
    "    description='Risk Tolerance:',\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='.2%',\n",
    "    tooltip='Lower = more cautious. Higher = more aggressive.',\n",
    "    layout=wide_layout\n",
    ")\n",
    "\n",
    "# --- Add Button and Output Area for Feedback ---\n",
    "\n",
    "# Button to confirm the upload and parameter settings\n",
    "confirm_button = widgets.Button(\n",
    "    description=\"Confirm Selections\",\n",
    "    button_style='success',\n",
    "    tooltip='Click to confirm your file and settings',\n",
    "    icon='check',\n",
    "    layout=button_layout\n",
    ")\n",
    "\n",
    "# Output widget to display feedback messages (success or error)\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# --- Define the logic for the button click ---\n",
    "def on_confirm_button_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output(wait=True) # Clear previous messages\n",
    "\n",
    "        # Add a try-except block for robust error handling\n",
    "        try:\n",
    "            # Check if a file has been uploaded\n",
    "            if not uploader.value:\n",
    "                print(\"❌ Error: Please upload a data file before confirming.\")\n",
    "                return\n",
    "\n",
    "            # Access the uploaded file's data and name\n",
    "            uploaded_file_dict = uploader.value[0]\n",
    "            original_name = uploaded_file_dict['name']\n",
    "\n",
    "            # Create the new filename with the current date\n",
    "            date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            base_name, extension = os.path.splitext(original_name)\n",
    "            new_filename_with_date = f\"{base_name}_{date_str}{extension}\"\n",
    "\n",
    "            # Store the data and new filename in global variables\n",
    "            global uploaded_data_content, confirmed_filename\n",
    "            uploaded_data_content = io.BytesIO(uploaded_file_dict['content'])\n",
    "            confirmed_filename = new_filename_with_date\n",
    "\n",
    "            # Provide success feedback to the user\n",
    "            print(f\"✅ Success! File '{original_name}' is ready for analysis.\")\n",
    "            print(f\"   It will be referred to as '{confirmed_filename}' in this session.\")\n",
    "            print(\"\\nYou may now proceed to the next cell.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs, print it clearly for debugging.\n",
    "            print(\"❌ An unexpected error occurred. Please check the details below:\")\n",
    "            print(f\"   Error Type: {type(e).__name__}\")\n",
    "            print(f\"   Error Details: {e}\")\n",
    "\n",
    "\n",
    "# Link the function to the button's click event\n",
    "confirm_button.on_click(on_confirm_button_clicked)\n",
    "\n",
    "# --- Display the final Control Panel using a VBox and Accordion for a structured layout ---\n",
    "# We use an Accordion to tuck away the more advanced settings, simplifying the UI.\n",
    "advanced_settings = widgets.Accordion(\n",
    "    children=[widgets.VBox([prior_selector, risk_slider])],\n",
    "    selected_index=None # Start with the accordion closed\n",
    ")\n",
    "advanced_settings.set_title(0, 'Advanced Settings (Prior & Risk)')\n",
    "advanced_settings.layout = wide_layout\n",
    "\n",
    "\n",
    "# The main VBox organizes all the elements vertically and adds a border.\n",
    "control_panel_layout = widgets.VBox([\n",
    "    widgets.HTML(\"<h2 style='font-family: Arial, sans-serif;'>Step 1: Configure Your Test</h2>\"),\n",
    "    widgets.HTML(\"<b style='font-family: Arial, sans-serif;'>Upload your data file and adjust the settings below, then click Confirm.</b>\"),\n",
    "    uploader,\n",
    "    advanced_settings,\n",
    "    confirm_button,\n",
    "    output_area\n",
    "], layout=widgets.Layout(\n",
    "    border='1px solid #ccc',\n",
    "    padding='15px',\n",
    "    border_radius='8px',\n",
    "    margin='10px 0'\n",
    "))\n",
    "\n",
    "display(control_panel_layout)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b54c2311f245adc",
   "metadata": {},
   "source": [
    "#### 3. Data Loading and Validation\n",
    "This cell handles the critical first step of loading and validating the A/B test data. The code will read the specified Excel file, confirm its existence, and verify that it contains the necessary columns for the analysis: variant, reach, and conversion. This ensures the data is correctly structured before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "id": "5f1361b781a403a8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --- Define Expected Data Structure ---\n",
    "REQUIRED_COLUMNS = ['variant', 'reach', 'conversion']\n",
    "\n",
    "# --- Validation and Loading Logic ---\n",
    "# First, check if the 'uploaded_data_content' variable exists.\n",
    "# This variable is created only when you click \"Confirm Selections\" in the Control Panel.\n",
    "if 'uploaded_data_content' not in locals():\n",
    "    print(\"❌ Error: No data file has been confirmed yet.\")\n",
    "    print(\"   Please go back to the Control Panel, upload a file, and click 'Confirm Selections'.\")\n",
    "\n",
    "else:\n",
    "    # If the data exists, try to load and validate it.\n",
    "    try:\n",
    "        print(f\"Attempting to load data from '{confirmed_filename}'...\")\n",
    "\n",
    "        # Load the data from the in-memory variable into a pandas DataFrame.\n",
    "        # We use 'uploaded_data_content' instead of a file path.\n",
    "        df = pd.read_excel(uploaded_data_content)\n",
    "\n",
    "        # Check if all the required columns are in the DataFrame.\n",
    "        if all(col in df.columns for col in REQUIRED_COLUMNS):\n",
    "            # If validation is successful, print a confirmation and the data's head.\n",
    "            print(\"✅ Success: File loaded and validated.\")\n",
    "            print(\"\\n--- First 5 Rows of the Dataset ---\")\n",
    "            print(df.head().to_string(index=False))\n",
    "        else:\n",
    "            # If columns are missing, identify them and report the error.\n",
    "            missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
    "            print(f\"---\")\n",
    "            print(f\"❌ Error: Missing Columns.\")\n",
    "            print(f\"The file '{confirmed_filename}' was loaded, but is missing required columns.\")\n",
    "            print(f\"   - Missing column(s): {missing_cols}\")\n",
    "            print(f\"   - Please ensure the file contains all of the following columns: {REQUIRED_COLUMNS}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any other potential errors during the file reading process.\n",
    "        print(f\"---\")\n",
    "        print(f\"❌ An unexpected error occurred while reading the file: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abd7b472985bcbb7",
   "metadata": {},
   "source": [
    "#### 4. Calculation of Posterior Parameters\n",
    "Here, we perform the core Bayesian update for our A/B test. 🧪"
   ]
  },
  {
   "cell_type": "code",
   "id": "34600db8b6eeee06",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# First, check if the DataFrame 'df' exists from the previous step.\n",
    "if 'df' not in locals():\n",
    "    print(\"❌ Error: DataFrame 'df' not found.\")\n",
    "    print(\"   Please run the 'Data Loading and Validation' cell successfully before proceeding.\")\n",
    "else:\n",
    "    try:\n",
    "        # Get the selected prior values (alpha, beta) from the Control Panel widget\n",
    "        PRIOR_ALPHA, PRIOR_BETA = prior_selector.value\n",
    "\n",
    "        # --- Apply the Beta-Binomial Conjugate Update Rule ---\n",
    "        # posterior_alpha = prior_alpha + number_of_successes (conversions)\n",
    "        # posterior_beta = prior_beta + number_of_failures (reach - conversions)\n",
    "        df['posterior_alpha'] = PRIOR_ALPHA + df['conversion']\n",
    "        df['posterior_beta'] = PRIOR_BETA + (df['reach'] - df['conversion'])\n",
    "\n",
    "        # --- Display the Updated DataFrame ---\n",
    "        # Show the DataFrame with the newly calculated posterior parameters.\n",
    "        print(\"✅ Success: Posterior parameters calculated.\")\n",
    "        print(\"\\n--- DataFrame with Updated Posterior Parameters ---\")\n",
    "        display_cols = ['variant', 'reach', 'conversion', 'posterior_alpha', 'posterior_beta']\n",
    "        print(df[display_cols].to_string(index=False))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred: {e}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "339d27af55852f67",
   "metadata": {},
   "source": [
    "#### 5. Generation of the Posterior Plot\n",
    "\n",
    "This cell generates the most important visualization for our analysis: the posterior probability distributions. The ridgeline plot is used for better readability when comparing multiple variants."
   ]
  },
  {
   "cell_type": "code",
   "id": "daafde3df8f8301",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# First, check if the DataFrame 'df' with posterior parameters exists.\n",
    "if 'df' in locals() and 'posterior_alpha' in df.columns:\n",
    "    try:\n",
    "        # --- 1. Setup for Ridgeline Plot ---\n",
    "        # We sort by the posterior mean to have a more organized plot.\n",
    "        # The posterior mean is the average of the distribution.\n",
    "        if 'posterior_mean' not in df.columns:\n",
    "             df['posterior_mean'] = df['posterior_alpha'] / (df['posterior_alpha'] + df['posterior_beta'])\n",
    "\n",
    "        sorted_df = df.sort_values('posterior_mean', ascending=False)\n",
    "\n",
    "        # Create a figure and axes for the plot.\n",
    "        fig, ax = plt.subplots(figsize=(12, 2 + len(sorted_df) * 0.7))\n",
    "\n",
    "        # --- 1a. Create a color palette ---\n",
    "        # Use a colormap to get a unique color for each variant.\n",
    "        colors = plt.cm.viridis(np.linspace(0.1, 0.9, len(sorted_df)))\n",
    "\n",
    "        # --- 2. Dynamically Determine X-axis Range ---\n",
    "        # Ensure all distributions and their credible intervals are fully visible.\n",
    "        max_x = 0\n",
    "        for i, row in sorted_df.iterrows():\n",
    "            percentile_999 = stats.beta.ppf(0.999, row['posterior_alpha'], row['posterior_beta'])\n",
    "            if percentile_999 > max_x:\n",
    "                max_x = percentile_999\n",
    "        x = np.linspace(0, max_x * 1.05, 1000)\n",
    "\n",
    "        # --- 3. Plot Each Variant as a Ridge ---\n",
    "        y_offset_step = 0.8  # Controls vertical spacing between ridges\n",
    "\n",
    "        for i, (row, color) in enumerate(zip(sorted_df.itertuples(), colors)):\n",
    "            y_offset = i * y_offset_step\n",
    "\n",
    "            # Calculate the Probability Density Function (PDF)\n",
    "            pdf = stats.beta.pdf(x, row.posterior_alpha, row.posterior_beta)\n",
    "\n",
    "            # Plot the main distribution curve with a label for the legend.\n",
    "            ax.plot(x, pdf + y_offset, color=color, lw=1.5, label=row.variant)\n",
    "\n",
    "            # Add a light fill for the entire distribution\n",
    "            ax.fill_between(x, y_offset, pdf + y_offset, alpha=0.2, color=color)\n",
    "\n",
    "            # --- 4. Calculate and Shade the 95% Credible Interval ---\n",
    "            # This interval contains the true conversion rate with 95% probability.\n",
    "            ci_low, ci_high = stats.beta.ppf([0.025, 0.975], row.posterior_alpha, row.posterior_beta)\n",
    "\n",
    "            # Create a mask for the x-values within the credible interval\n",
    "            ci_mask = (x >= ci_low) & (x <= ci_high)\n",
    "\n",
    "            # Add a darker shade on top for the 95% credible interval\n",
    "            ax.fill_between(x[ci_mask], y_offset, pdf[ci_mask] + y_offset, alpha=0.4, color=color)\n",
    "\n",
    "        # --- 5. Finalize and Display the Plot ---\n",
    "        from matplotlib.patches import Patch\n",
    "\n",
    "        # Clean up the plot aesthetics\n",
    "        ax.set_title('Posterior Distributions of Conversion Rates', fontsize=16)\n",
    "        ax.set_xlabel('Conversion Rate', fontsize=12)\n",
    "        ax.set_yticks([]) # Hide y-axis ticks as they are not meaningful here\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "\n",
    "        # Create and display a custom legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        # Add \"Variant\" prefix to each label\n",
    "        new_labels = [f'Variant {label}' for label in labels]\n",
    "\n",
    "        # Create a proxy artist for the shaded area to include in the legend\n",
    "        ci_patch = Patch(color='gray', alpha=0.4, label='95% Credible Interval')\n",
    "        handles.append(ci_patch)\n",
    "        new_labels.append('95% Credible Interval')\n",
    "\n",
    "        # Position the legend above the plot in the top-right corner with a smaller font\n",
    "        fig.legend(handles, new_labels, title=\"Legend\", bbox_to_anchor=(0.98, 0.98), loc='upper right', fontsize='small')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout to make space for the title and legend\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred while generating the plot: {e}\")\n",
    "else:\n",
    "    print(\"❌ Error: DataFrame 'df' with posterior parameters not found.\")\n",
    "    print(\"   Please run the previous cells successfully before proceeding.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "236e80b258b92449",
   "metadata": {},
   "source": [
    "#### 6. # Monte Carlo Simulation\n",
    "\n",
    "Run this cell to start the Monte Carlo Simulation and create 100.000 random samples."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb15b530dd5c025d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --- 1. Simulation Setup ---\n",
    "# Define the number of random samples to generate for each variant's distribution.\n",
    "# A larger number of samples leads to more stable and accurate estimates of our metrics.\n",
    "N_SAMPLES = 100000\n",
    "\n",
    "# We will store the generated samples in a dictionary, with variant names as keys.\n",
    "posterior_samples = {}\n",
    "\n",
    "\n",
    "# --- 2. Run Simulation ---\n",
    "# Check if the dataframe 'df' with posterior parameters exists to avoid errors.\n",
    "if 'df' in locals() and 'posterior_alpha' in df.columns:\n",
    "\n",
    "    # Iterate over each variant (row) in the DataFrame.\n",
    "    for i, row in df.iterrows():\n",
    "        variant_name = row['variant']\n",
    "        p_alpha = row['posterior_alpha']\n",
    "        p_beta = row['posterior_beta']\n",
    "\n",
    "        # Generate N_SAMPLES from the Beta distribution defined by the variant's\n",
    "        # posterior parameters. Each sample represents a plausible \"true\"\n",
    "        # conversion rate for that variant, according to our model.\n",
    "        samples = stats.beta.rvs(a=p_alpha, b=p_beta, size=N_SAMPLES)\n",
    "\n",
    "        # Store the resulting array of samples in our dictionary.\n",
    "        posterior_samples[variant_name] = samples\n",
    "\n",
    "    # --- 3. Output: Simulation Summary ---\n",
    "    # The simulation is complete. The following is a summary of the process.\n",
    "    print(\"--- Monte Carlo Simulation Summary ---\")\n",
    "    print(f\"✅ Simulation completed successfully.\")\n",
    "    print(f\"   - Samples generated per variant: {N_SAMPLES:,}\")\n",
    "    print(f\"   - Variants simulated: {list(posterior_samples.keys())}\")\n",
    "\n",
    "    print(\"\\nData Preview (first 3 samples for each variant):\")\n",
    "    for variant, samples in posterior_samples.items():\n",
    "        preview = [round(s, 6) for s in samples[:3]]\n",
    "        print(f\"  - {variant}: {preview}\")\n",
    "\n",
    "    print(\"\\nThe 'posterior_samples' dictionary is now ready for metric calculation in the next cell.\")\n",
    "\n",
    "else:\n",
    "    # This message will only be displayed if the prerequisite DataFrame is not found.\n",
    "    print(\"Error: DataFrame 'df' with posterior parameters not found.\")\n",
    "    print(\"Please ensure the data loading (Cell 5) and posterior calculation (Cell 7) were executed successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "667f4aa713eb94f4",
   "metadata": {},
   "source": [
    "#### 7. Calculation and Presentation of Metrics\n",
    "This is the final calculation step where we translate our simulation results into actionable business metrics. 🏆"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4fd4446b762dcdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# First, check if the simulation data from the previous step exists.\n",
    "if 'posterior_samples' in locals():\n",
    "    try:\n",
    "        # --- Get the Risk Threshold from the Control Panel ---\n",
    "        # This line was missing. It retrieves the value set by the designer.\n",
    "        RISK_THRESHOLD = risk_slider.value\n",
    "\n",
    "        # --- 1. Combine Simulation Samples into a DataFrame ---\n",
    "        samples_df = pd.DataFrame(posterior_samples)\n",
    "        samples_df['max_conversion_rate'] = samples_df.max(axis=1)\n",
    "        variant_names = list(posterior_samples.keys())\n",
    "\n",
    "        # --- 2. Calculate 'Probability to be Best' and 'Expected Loss' ---\n",
    "        results = []\n",
    "        for variant in variant_names:\n",
    "            prob_best = (samples_df[variant] == samples_df['max_conversion_rate']).mean()\n",
    "            loss = samples_df['max_conversion_rate'] - samples_df[variant]\n",
    "            expected_loss = loss.mean()\n",
    "            results.append({\n",
    "                \"Variant\": variant,\n",
    "                \"Probability to be Best\": prob_best,\n",
    "                \"Expected Loss (Risk)\": expected_loss\n",
    "            })\n",
    "\n",
    "        # --- 3. Format and Display Results as a Table ---\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "        # Create the 'Decision Guide' column based on the threshold\n",
    "        conditions = [\n",
    "            results_df['Expected Loss (Risk)'] > RISK_THRESHOLD,\n",
    "            results_df['Expected Loss (Risk)'] < RISK_THRESHOLD\n",
    "        ]\n",
    "        choices = ['Above Threshold', 'Below Threshold']\n",
    "        results_df['Decision Guide'] = np.select(conditions, choices, default='Equals Threshold')\n",
    "\n",
    "        # Add other necessary columns from the main 'df' for the final report\n",
    "        if 'df' in locals():\n",
    "            results_df = pd.merge(results_df, df[['variant', 'posterior_mean']], left_on='Variant', right_on='variant', how='left').drop('variant', axis=1)\n",
    "\n",
    "        # Sort the DataFrame by the lowest risk\n",
    "        results_df = results_df.sort_values(by=\"Expected Loss (Risk)\")\n",
    "\n",
    "        # --- 4. Final Styling ---\n",
    "        styled_df = results_df.style.format({\n",
    "            \"Probability to be Best\": \"{:.2%}\",\n",
    "            \"Expected Loss (Risk)\": \"{:.4%}\"\n",
    "        }).set_properties(**{'text-align': 'center'}) \\\n",
    "        .set_caption(f\"🏆 Bayesian A/B Test Results (Risk Threshold: {RISK_THRESHOLD:.1%})\") \\\n",
    "        .hide(axis=\"index\")\n",
    "\n",
    "        # Display the final, styled table\n",
    "        display(styled_df)\n",
    "        print(\"\\n(A variant with risk 'Below Threshold' is generally considered a safe choice.)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    print(\"❌ Error: The 'posterior_samples' dictionary was not found.\")\n",
    "    print(\"   Please ensure the Monte Carlo Simulation cell was executed successfully.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84061d34cea7335d",
   "metadata": {},
   "source": [
    "#### 8. Automated Conclusion Logic\n",
    "This final, automated cell translates our statistical results into a clear business recommendation with key metrics. 🎯"
   ]
  },
  {
   "cell_type": "code",
   "id": "62365a9c7c89e4db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# First, check if the results_df DataFrame from the previous step exists.\n",
    "if 'results_df' not in locals():\n",
    "    print(\"❌ Error: The 'results_df' DataFrame was not found.\")\n",
    "    print(\"   Please run the 'Calculation and Presentation of Metrics' cell successfully before proceeding.\")\n",
    "else:\n",
    "    try:\n",
    "        # --- Get Risk Threshold from Control Panel ---\n",
    "        RISK_THRESHOLD = risk_slider.value\n",
    "\n",
    "        # --- Ensure all necessary data is in results_df for the report ---\n",
    "        if 'df' in locals():\n",
    "            required_cols = ['posterior_mean', 'reach', 'conversion']\n",
    "            missing_cols = [col for col in required_cols if col not in results_df.columns]\n",
    "\n",
    "            if missing_cols:\n",
    "                cols_to_merge = ['variant'] + missing_cols\n",
    "                if all(col in df.columns for col in cols_to_merge):\n",
    "                    results_df = pd.merge(\n",
    "                        results_df,\n",
    "                        df[cols_to_merge],\n",
    "                        left_on='Variant',\n",
    "                        right_on='variant',\n",
    "                        how='left'\n",
    "                    ).drop(columns='variant', errors='ignore')\n",
    "\n",
    "        # --- Extract Key Information ---\n",
    "        best_candidate = results_df.iloc[0]\n",
    "        is_winner = best_candidate['Expected Loss (Risk)'] < RISK_THRESHOLD\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # Prepare list that will feed df_uplift (always created)\n",
    "        uplift_rows = [{\n",
    "            \"Compared To\": f\"Variant '{best_candidate['Variant']}' (Winner)\",\n",
    "            \"Expected Uplift\": 0.0,\n",
    "            \"Conversion Gain\": 0,\n",
    "            \"Total Expected\": int(best_candidate['conversion'])\n",
    "        }]\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # --- Build the Markdown Report String ---\n",
    "        markdown_report = \"\"\n",
    "\n",
    "        # 1. Verdict\n",
    "        if is_winner:\n",
    "            markdown_report += f\"## ✅ Verdict: Test Concluded. Deploy Variant '{best_candidate['Variant']}'.\\n\"\n",
    "        else:\n",
    "            markdown_report += f\"## ⚠️ Verdict: Test Inconclusive. Collect More Data.\\n\"\n",
    "        markdown_report += \"---\\n\"\n",
    "\n",
    "        # 2. Stakeholder Summary\n",
    "        markdown_report += \"### Summary for Stakeholders\\n\"\n",
    "        if is_winner:\n",
    "            summary_text = (\n",
    "                f\"The analysis confidently recommends deploying **Variant \"\n",
    "                f\"'{best_candidate['Variant']}'**. It has the highest chance of being the \"\n",
    "                f\"best option, and the risk of choosing it is well below our safety limit \"\n",
    "                f\"of {RISK_THRESHOLD:.1%}. The table in the final section shows the expected \"\n",
    "                f\"performance increase against all other variants.\"\n",
    "            )\n",
    "        else:\n",
    "            summary_text = (\n",
    "                \"The results are not yet clear enough to make a confident decision. \"\n",
    "                \"Our best option still has a risk level higher than our limit. \"\n",
    "                \"We recommend collecting more data to get a clearer winner.\"\n",
    "            )\n",
    "        markdown_report += summary_text + \"\\n\\n---\\n\"\n",
    "\n",
    "        # 3. Key Metrics Explained\n",
    "        markdown_report += \"### Key Metrics Explained\\n\"\n",
    "        prob_best_str = f\"{best_candidate['Probability to be Best']:.1%}\"\n",
    "        markdown_report += (\n",
    "            f\"**🔹 Probability to be Best: {prob_best_str}**\\n\"\n",
    "            f\"   - *What it means:* This is the probability that Variant \"\n",
    "            f\"'{best_candidate['Variant']}' is truly the best option among all variants \"\n",
    "            f\"tested. A higher percentage means more confidence in it being the winner.\\n\\n\"\n",
    "        )\n",
    "        risk_str = f\"{best_candidate['Expected Loss (Risk)']:.4%}\"\n",
    "        markdown_report += (\n",
    "            f\"**🔹 Risk (Expected Loss): {risk_str}**\\n\"\n",
    "            f\"   - *What it means:* This is the 'cost of being wrong.' It represents the \"\n",
    "            f\"average potential drop in conversion rate you would risk by choosing this \"\n",
    "            f\"variant if another one was secretly better. A lower risk is better.\\n\\n\"\n",
    "        )\n",
    "        markdown_report += (\n",
    "            \"**🔹 Expected Uplift**\\n\"\n",
    "            \"   - *What it means:* This is the expected percentage increase in the \"\n",
    "            \"conversion rate of the winning variant compared to another. The table below \"\n",
    "            \"shows this uplift and the **potential gain in conversions** for the same \"\n",
    "            \"number of visitors.\\n\"\n",
    "        )\n",
    "\n",
    "        # 4. Expected Uplift Table\n",
    "        required_cols_for_table = ['posterior_mean', 'reach', 'conversion']\n",
    "        if is_winner and len(results_df) > 1 and all(col in results_df.columns for col in required_cols_for_table):\n",
    "            markdown_report += \"\\n---\\n\"\n",
    "            markdown_report += \"### Expected Uplift vs. Other Variants\\n\"\n",
    "            markdown_report += \"| Compared To | Expected Uplift | Potential Conversion Gain | Total Expected Conversions |\\n\"\n",
    "            markdown_report += \"|:---|:---|:---|:---|\\n\"\n",
    "\n",
    "            winner_conversions = best_candidate['conversion']\n",
    "            markdown_report += (\n",
    "                f\"| **Variant '{best_candidate['Variant']}' (Winner)** | **-** | \"\n",
    "                f\"**{int(winner_conversions)} conversions (actual)** | \"\n",
    "                f\"**{int(winner_conversions)}** |\\n\"\n",
    "            )\n",
    "\n",
    "            for _, other_variant in results_df.iloc[1:].iterrows():\n",
    "                uplift = (\n",
    "                    (best_candidate['posterior_mean'] - other_variant['posterior_mean'])\n",
    "                    / other_variant['posterior_mean']\n",
    "                )\n",
    "                conversion_gain = (\n",
    "                    (best_candidate['posterior_mean'] - other_variant['posterior_mean'])\n",
    "                    * best_candidate['reach']\n",
    "                )\n",
    "                total_expected = winner_conversions + conversion_gain\n",
    "\n",
    "                # ---- add row to Markdown table ----\n",
    "                markdown_report += (\n",
    "                    f\"| Variant '{other_variant['Variant']}' | +{uplift:.2%} | \"\n",
    "                    f\"**+{int(round(conversion_gain, 0))}** more conversions | \"\n",
    "                    f\"{int(round(total_expected, 0))} |\\n\"\n",
    "                )\n",
    "\n",
    "                # ---- save same values for df_uplift ----\n",
    "                uplift_rows.append({\n",
    "                    \"Compared To\": f\"Variant '{other_variant['Variant']}'\",\n",
    "                    \"Expected Uplift\": round(float(uplift), 6),\n",
    "                    \"Conversion Gain\": int(round(conversion_gain, 0)),\n",
    "                    \"Total Expected\": int(round(total_expected, 0))\n",
    "                })\n",
    "            markdown_report += \"\\n\"\n",
    "\n",
    "        # --- Display Markdown report ---\n",
    "        display(Markdown(markdown_report))\n",
    "\n",
    "        # --- Create df_uplift DataFrame and (optionally) persist -----------------\n",
    "        df_uplift = pd.DataFrame(uplift_rows)\n",
    "        # df_uplift.to_csv('expected_uplift.csv', index=False)   # opcional\n",
    "        # df_uplift.to_pickle('expected_uplift.pkl')             # opcional\n",
    "        print(\"✅ df_uplift criado com\", len(df_uplift), \"linhas.\")\n",
    "        display(df_uplift)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred while generating the report: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8898f906103e6bff",
   "metadata": {},
   "source": [
    "# Creating the report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d4e8b9470db50",
   "metadata": {},
   "source": [
    "### Project Configuration Panel\n",
    "\n",
    "This cell launches a compact three‑step wizard that\n",
    "\n",
    "1. **Validates your Gemini API key**,\n",
    "2. **Lists compatible Gemini models for you to pick**, and\n",
    "3. **Accepts a Markdown project file** while recording your preferred language.\n",
    "\n",
    "Once the steps are complete, the file’s content is loaded into a DataFrame called `project_description_df`, making it immediately available to the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "97bf0351a7dc4086",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Create Layouts and Output Areas ---\n",
    "api_step_box = widgets.VBox()\n",
    "model_step_box = widgets.VBox(layout={'display': 'none'})     # Hidden initially\n",
    "upload_step_box = widgets.VBox(layout={'display': 'none'})    # Hidden initially\n",
    "final_output_area = widgets.Output()\n",
    "\n",
    "# --- Step 1: API Key Validation ---\n",
    "\n",
    "api_key_input = widgets.Password(description='Chave de API Gemini:')\n",
    "validate_api_button = widgets.Button(description=\"Validar Chave de API\", button_style='info')\n",
    "\n",
    "def on_validate_api_clicked(b):\n",
    "    \"\"\"Validates the API key and reveals the model selector.\"\"\"\n",
    "    with final_output_area:\n",
    "        clear_output(wait=True)\n",
    "        print(\"⏳ Validando Chave de API...\")\n",
    "        try:\n",
    "            genai.configure(api_key=api_key_input.value)\n",
    "            available_models = [\n",
    "                m.name for m in genai.list_models()\n",
    "                if 'generateContent' in m.supported_generation_methods\n",
    "            ]\n",
    "            filtered_models = [m for m in available_models if 'models/gemini' in m]\n",
    "\n",
    "            if not filtered_models:\n",
    "                print(\"❌ Erro: A Chave de API é válida, mas não foram encontrados modelos Gemini compatíveis.\")\n",
    "                return\n",
    "\n",
    "            print(\"✅ Sucesso: Chave de API validada.\")\n",
    "            print(f\"   - Encontrados {len(filtered_models)} modelos compatíveis.\")\n",
    "\n",
    "            model_selector.options = filtered_models\n",
    "            api_step_box.layout.display = 'none'\n",
    "            model_step_box.layout.display = 'flex'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro: Não foi possível validar a Chave de API. Detalhes: {e}\")\n",
    "\n",
    "validate_api_button.on_click(on_validate_api_clicked)\n",
    "api_step_box.children = [\n",
    "    widgets.HTML(\"<h3>Passo 1: Configurar API</h3>\"),\n",
    "    api_key_input,\n",
    "    validate_api_button\n",
    "]\n",
    "\n",
    "# --- Step 2: Model Selection ---\n",
    "\n",
    "model_selector = widgets.Dropdown(description='Selecionar Modelo:')\n",
    "confirm_model_button = widgets.Button(description=\"Confirmar Modelo\", button_style='info')\n",
    "\n",
    "def on_confirm_model_clicked(b):\n",
    "    \"\"\"Confirms the model choice and reveals the file uploader.\"\"\"\n",
    "    with final_output_area:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"✅ Modelo selecionado: {model_selector.value}\")\n",
    "\n",
    "    model_step_box.layout.display = 'none'\n",
    "    upload_step_box.layout.display = 'flex'\n",
    "\n",
    "confirm_model_button.on_click(on_confirm_model_clicked)\n",
    "model_step_box.children = [\n",
    "    widgets.HTML(\"<h3>Passo 2: Selecionar Modelo de IA</h3>\"),\n",
    "    model_selector,\n",
    "    confirm_model_button\n",
    "]\n",
    "\n",
    "# --- Step 3: File Upload and DataFrame Creation ---\n",
    "\n",
    "# Idioma (novo seletor)\n",
    "language_selector = widgets.Dropdown(\n",
    "    description='Idioma:',\n",
    "    options=[\n",
    "        'English',           # 1\n",
    "        'Português',         # 2\n",
    "        'Español',           # 3\n",
    "        'Français',          # 4\n",
    "        'Deutsch',           # 5\n",
    "        'Italiano',          # 6\n",
    "        '日本語',             # 7 – Japanese\n",
    "        '中文',               # 8 – Chinese\n",
    "        'हिन्दी',            # 9 – Hindi\n",
    "        'العربية'            # 10 – Arabic\n",
    "    ],\n",
    "    value='English'\n",
    ")\n",
    "\n",
    "project_uploader = widgets.FileUpload(\n",
    "    accept='.md',\n",
    "    description='Carregar Ficheiro do Projeto'\n",
    ")\n",
    "create_df_button = widgets.Button(\n",
    "    description=\"Carregar e Criar DataFrame\",\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "def on_create_df_clicked(b):\n",
    "    \"\"\"Handles file upload, reads its content, and creates a DataFrame.\"\"\"\n",
    "    with final_output_area:\n",
    "        clear_output(wait=True)\n",
    "        print(\"⏳ Processando ficheiro carregado...\")\n",
    "\n",
    "        if not project_uploader.value:\n",
    "            print(\"❌ Erro: Por favor, carregue um ficheiro.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            uploaded_file_dict = project_uploader.value[0]\n",
    "            project_content = uploaded_file_dict['content'].tobytes().decode('utf-8')\n",
    "\n",
    "            global project_description_df\n",
    "            project_description_df = pd.DataFrame([{'content': project_content}])\n",
    "\n",
    "            print(\"✅ Sucesso! O conteúdo do ficheiro do projeto foi lido e armazenado em `project_description_df`.\")\n",
    "            display(project_description_df)\n",
    "            print(\"\\n🚀 Tudo pronto. Pode prosseguir para a próxima célula.\")\n",
    "            upload_step_box.layout.display = 'none'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ocorreu um erro: {e}\")\n",
    "\n",
    "create_df_button.on_click(on_create_df_clicked)\n",
    "upload_step_box.children = [\n",
    "    widgets.HTML(\"<h3>Passo 3: Carregar Ficheiro do Projeto e Selecionar Idioma</h3>\"),\n",
    "    language_selector,\n",
    "    project_uploader,\n",
    "    create_df_button\n",
    "]\n",
    "\n",
    "# --- Display the Final Control Panel ---\n",
    "full_panel = widgets.VBox([\n",
    "    widgets.HTML(\"<h2 style='font-family: Arial, sans-serif;'>Configuração do Projeto</h2>\"),\n",
    "    api_step_box,\n",
    "    model_step_box,\n",
    "    upload_step_box,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    final_output_area\n",
    "], layout={'border': '1px solid #ccc', 'padding': '15px', 'border_radius': '8px'})\n",
    "\n",
    "display(full_panel)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f05b39d714640f06",
   "metadata": {},
   "source": [
    "### AI‑Powered Narrative Analysis\n",
    "\n",
    "This cell brings together your project description and A/B test results, asks Gemini to craft a SCQA‑based story, and stores the slide‑by‑slide output in a DataFrame called `presentation_df` for the presentation builder that follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3f5c23e34c3f1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "# This cell uses the LLM to analyze the test results and project context,\n",
    "# then structures the output into a DataFrame that will be used to build\n",
    "# the presentation in the next cell.\n",
    "\n",
    "def analyze_and_create_dataframe():\n",
    "    \"\"\"\n",
    "    Uses the LLM to perform SCQA analysis and generate a structured DataFrame\n",
    "    for the presentation.\n",
    "    \"\"\"\n",
    "    print(\"⏳ Starting AI-powered analysis...\")\n",
    "\n",
    "    # --- 1. Pre-flight Checks ---\n",
    "    if 'results_df' not in globals():\n",
    "        print(\"❌ Error: The 'results_df' DataFrame was not found. Please run the previous analysis cells first.\")\n",
    "        return\n",
    "    if 'project_description_df' not in globals():\n",
    "        print(\"❌ Error: 'project_description_df' not found. Please run the previous cell to upload the project file.\")\n",
    "        return\n",
    "    if 'model_selector' not in globals() or model_selector.disabled or not model_selector.value.startswith('models/'):\n",
    "        print(\"❌ Error: An AI model has not been selected. Please validate your API key in the panel above to select a model.\")\n",
    "        return\n",
    "\n",
    "    print(\"✅ Pre-flight checks passed.\")\n",
    "\n",
    "    try:\n",
    "        # --- 2. Gather All Information ---\n",
    "        print(\"   - Reading project context from DataFrame...\")\n",
    "        project_context = project_description_df['content'].iloc[0]\n",
    "        results_data_string = globals()['results_df'].to_markdown(index=False)\n",
    "\n",
    "        # --- 3. Re-engineered Prompt for Structured Analysis ---\n",
    "        prompt_instructions = \"\"\"\n",
    "        **Primary Goal**: Analyze the provided A/B test data and project context to create a structured presentation plan.\n",
    "\n",
    "        **Your Task**:\n",
    "        1.  **Synthesize the Narrative**: Read the `Project Context` and `A/B Test Data Results`. Formulate a coherent story using the SCQA (Situation, Complication, Question, Answer) framework.\n",
    "        2.  **Populate the JSON Structure**: Based on your SCQA analysis, fill out the JSON object below.\n",
    "        3.  **Output ONLY the JSON object.** Do not include any other text, markdown, or explanations.\n",
    "\n",
    "        **Rules for Analysis**:\n",
    "        * **Grounding is Mandatory**: Every value in the JSON MUST be derived directly from the provided context and data.\n",
    "        * **No Hallucination**: DO NOT invent data. If a metric (e.g., revenue) is not in the source files, use an available metric (like 'conversions') or state it's not available.\n",
    "        * **Plain Text Only**: The text in the \"title\" and \"body\" fields must be clean, plain text ready for a presentation. **Do not include any markdown formatting like `**`, `*`, `-`, or `###`.**\n",
    "        * **Visual Data**: For the `visual_data` field, provide the specific numbers needed to create the chart.\n",
    "\n",
    "        **JSON Output Structure (Follow these content instructions strictly)**:\n",
    "        ```json\n",
    "        [\n",
    "          {\n",
    "            \"slide\": 1,\n",
    "            \"title\": \"Our Context and the Key Question\",\n",
    "            \"body\": \"For this slide's body, create a cohesive paragraph of plain text. Start by describing the factual Situation (including the baseline). Follow this by explaining the Complication (the problem or opportunity). Conclude by formulating the Key Question that needs to be answered. Do not use markdown formatting like 'Situation:' or '**'.\",\n",
    "            \"visual_type\": \"delta_chart\",\n",
    "            \"visual_data\": {\"from\": 0.03, \"to\": 0.05, \"label\": \"Performance Gap\"}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 2,\n",
    "            \"title\": \"Our Recommendation to Answer the Question\",\n",
    "            \"body\": \"Present the direct, high-level answer that resolves the 'Key Question' from the previous slide. This is your main recommendation, the hypothesis that will be detailed in the following slides. Use plain text.\",\n",
    "            \"visual_type\": \"icon\",\n",
    "            \"visual_data\": {\"icon\": \"🏆\"}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 3,\n",
    "            \"title\": \"Variants Tested\",\n",
    "            \"body\": \"List each variant with a brief description as provided in the project context. Present them as a clean, multi-line string, each variant on a new line. Do not use markdown bullet points.\",\n",
    "            \"visual_type\": \"list\",\n",
    "            \"visual_data\": {}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 4,\n",
    "            \"title\": \"Key Evidence: Why This Recommendation Works\",\n",
    "            \"body\": \"Provide the top 2-3 evidence points that support your recommendation. Present them as a clean, multi-line string, with each point on a new line. Do not use markdown bullet points like '*' or '-'.\",\n",
    "            \"visual_type\": \"comparison\",\n",
    "            \"visual_data\": {}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 5,\n",
    "            \"title\": \"Risk Assessment of the Recommendation\",\n",
    "            \"body\": \"Analyze the risk (Expected Loss) and compare it with the defined tolerance. Mention risks from the context that support the decision. Use plain text.\",\n",
    "            \"visual_type\": \"gauge_meter\",\n",
    "            \"visual_data\": {\"value\": 0.0003, \"threshold\": 0.01}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 6,\n",
    "            \"title\": \"Expected Business Impact\",\n",
    "            \"body\": \"Translate the results into a tangible business outcome (e.g., conversion lift) that justifies the recommendation. Use plain text.\",\n",
    "            \"visual_type\": \"waterfall_chart\",\n",
    "            \"visual_data\": {}\n",
    "          },\n",
    "          {\n",
    "            \"slide\": 7,\n",
    "            \"title\": \"Immediate Action Plan\",\n",
    "            \"body\": \"Provide 2-3 clear, actionable next steps to implement the 'Answer'. Present them as a clean, multi-line string, with each point on a new line. Do not use markdown bullet points.\",\n",
    "            \"visual_type\": \"timeline\",\n",
    "            \"visual_data\": {}\n",
    "          }\n",
    "        ]\n",
    "        ```\n",
    "        \"\"\"\n",
    "\n",
    "        selected_language = language_selector.value\n",
    "        selected_model_name = model_selector.value\n",
    "        print(f\"   - Using AI model: {selected_model_name} in {selected_language}\")\n",
    "\n",
    "        final_prompt = f\"\"\"\n",
    "        **Source Material**\n",
    "\n",
    "        Here is the context and data you must use for your analysis. You are forbidden from using any information not present here.\n",
    "\n",
    "        **1. Project Context:**\n",
    "        ---\n",
    "        {project_context}\n",
    "        ---\n",
    "\n",
    "        **2. A/B Test Data Results:**\n",
    "        ---\n",
    "        {results_data_string}\n",
    "        ---\n",
    "\n",
    "        **Your Task**\n",
    "\n",
    "        Now, using ONLY the source material provided above, perform the following task.\n",
    "\n",
    "        **Language**: {selected_language}\n",
    "        **Instructions**: {prompt_instructions}\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 4. Call the Gemini API ---\n",
    "        print(\"   - Requesting analysis from Gemini API...\")\n",
    "        model = genai.GenerativeModel(selected_model_name)\n",
    "\n",
    "        generation_config = genai.types.GenerationConfig(\n",
    "            max_output_tokens=8192,\n",
    "            response_mime_type=\"application/json\"\n",
    "        )\n",
    "\n",
    "        response = model.generate_content(\n",
    "            final_prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "        print(\"   - Received structured analysis from Gemini.\")\n",
    "        report_text = response.text\n",
    "\n",
    "        # --- 5. Create and Display DataFrame ---\n",
    "        slides_data = json.loads(report_text)\n",
    "\n",
    "        global presentation_df\n",
    "        presentation_df = pd.DataFrame(slides_data)\n",
    "\n",
    "        print(\"\\n✅ Success! AI analysis complete. The `presentation_df` is ready for the next cell.\")\n",
    "        print(\"--- Presentation Plan DataFrame (First 5 Rows) ---\")\n",
    "        display(presentation_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred during AI analysis: {e}\")\n",
    "        if 'response' in locals():\n",
    "            print(\"\\n--- Raw AI Response for Debugging ---\")\n",
    "            print(response.text)\n",
    "\n",
    "# --- Automatically run the analysis ---\n",
    "analyze_and_create_dataframe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "421db9fc6e5bef63",
   "metadata": {},
   "source": [
    "### PowerPoint Slide Generation\n",
    "\n",
    "This cell takes the structured plan in `presentation_df` and automatically builds a tidy 16:9 PowerPoint deck—adding titles, body text, and data‑driven visuals like delta charts and risk gauges—then saves the file to a `reports/` folder for easy download.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "53c3b7a856bbcdae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import pandas as pd\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# This cell uses the 'presentation_df' as a blueprint to programmatically generate\n",
    "# a PowerPoint presentation, including data‑driven visuals.\n",
    "\n",
    "# --- Helper Functions for Chart Generation ---\n",
    "\n",
    "def create_bar_chart(visual_data, output_buffer):\n",
    "    \"\"\"Generates a simple bar chart and saves it to the buffer.\"\"\"\n",
    "    label = visual_data.get('label', 'Label')\n",
    "    value = visual_data.get('value', 0) * 100  # Convert to percentage\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    bars = ax.bar([label], [value], color='#4A90E2', width=0.5)\n",
    "    ax.set_ylabel('Conversion Rate (%)')\n",
    "    ax.set_ylim(0, max(10, value * 1.2))\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0,\n",
    "                yval + 0.5,\n",
    "                f'{yval:.1f}%',\n",
    "                ha='center',\n",
    "                va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_buffer, format='png', dpi=150, transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def create_delta_chart(visual_data, output_buffer):\n",
    "    \"\"\"Generates a two‑bar delta chart (from → to) and saves it to the buffer.\"\"\"\n",
    "    frm = visual_data.get('from', 0) * 100\n",
    "    to = visual_data.get('to', 0) * 100\n",
    "    label = visual_data.get('label', 'Delta')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    bars = ax.bar(['From', 'To'], [frm, to],\n",
    "                  color=['#4A90E2', '#F5A623'], width=0.5)\n",
    "    ax.set_ylabel('Conversion Rate (%)')\n",
    "    ax.set_ylim(0, max(10, to * 1.2))\n",
    "    ax.set_title(label)\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0,\n",
    "                yval + 0.5,\n",
    "                f'{yval:.1f}%',\n",
    "                ha='center',\n",
    "                va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_buffer, format='png', dpi=150, transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def create_gauge_meter(visual_data, output_buffer):\n",
    "    \"\"\"Generates a risk gauge meter and saves it to the buffer.\"\"\"\n",
    "    value = visual_data.get('value', 0) * 100\n",
    "    threshold = visual_data.get('threshold', 0.01) * 100\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 2.5))\n",
    "    ax.set_xlim(0, threshold * 2)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Draw the gauge arc\n",
    "    ax.add_patch(plt.Rectangle((0, 0), threshold, 0.2,\n",
    "                               color='#4CAF50', alpha=0.7))  # Green (Safe)\n",
    "    ax.add_patch(plt.Rectangle((threshold, 0), threshold, 0.2,\n",
    "                               color='#F44336', alpha=0.7))  # Red (Risk)\n",
    "\n",
    "    # Draw the pointer\n",
    "    ax.arrow(value, 0.4, 0, -0.15,\n",
    "             head_width=0.05 * threshold,\n",
    "             head_length=0.05,\n",
    "             fc='black',\n",
    "             ec='black')\n",
    "    ax.text(value, 0.45, f'Risk: {value:.3f}%',\n",
    "            ha='center', fontsize=12, weight='bold')\n",
    "    ax.text(threshold, -0.1, f'Threshold: {threshold:.1f}%',\n",
    "            ha='center', fontsize=10)\n",
    "\n",
    "    plt.box(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_buffer, format='png', dpi=150, transparent=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- Main Presentation Generation Function ---\n",
    "\n",
    "def create_presentation_from_dataframe():\n",
    "    \"\"\"Loops through the presentation_df and builds a PowerPoint slide for each row.\"\"\"\n",
    "    print(\"⏳ Starting PowerPoint generation...\")\n",
    "\n",
    "    # --- 1. Pre-flight Check ---\n",
    "    if 'presentation_df' not in globals() or not isinstance(presentation_df, pd.DataFrame):\n",
    "        print(\"❌ Error: The 'presentation_df' DataFrame was not found.\")\n",
    "        print(\"   Please run the 'AI-Powered Narrative Analysis' cell successfully before proceeding.\")\n",
    "        return\n",
    "\n",
    "    print(f\"✅ Found presentation plan with {len(presentation_df)} slides.\")\n",
    "\n",
    "    try:\n",
    "        # --- 2. Initialize Presentation ---\n",
    "        prs = Presentation()\n",
    "        prs.slide_width = Inches(10)\n",
    "        prs.slide_height = Inches(5.625)\n",
    "\n",
    "        # --- 3. Loop Through DataFrame and Create Slides ---\n",
    "        print(\"   - Building slides from the DataFrame...\")\n",
    "        for _, row in presentation_df.iterrows():\n",
    "            slide_title = row.get('title', 'No Title Provided')\n",
    "            slide_body = row.get('body', '')\n",
    "            visual_type = row.get('visual_type', 'none')\n",
    "            visual_data = row.get('visual_data', {})\n",
    "\n",
    "            slide_layout = prs.slide_layouts[6]  # Blank layout\n",
    "            slide = prs.slides.add_slide(slide_layout)\n",
    "\n",
    "            # --- Add Title ---\n",
    "            title_shape = slide.shapes.add_textbox(\n",
    "                Inches(0.5), Inches(0.2), Inches(9), Inches(0.75))\n",
    "            title_frame = title_shape.text_frame\n",
    "            title_frame.text = slide_title\n",
    "            p = title_frame.paragraphs[0]\n",
    "            p.font.bold = True\n",
    "            p.font.size = Pt(28)\n",
    "            title_frame.word_wrap = True\n",
    "\n",
    "            # --- Add Body Content ---\n",
    "            body_shape = slide.shapes.add_textbox(\n",
    "                Inches(0.5), Inches(1.0), Inches(5.5), Inches(4.0))\n",
    "            body_frame = body_shape.text_frame\n",
    "            body_frame.text = slide_body\n",
    "            p = body_frame.paragraphs[0]\n",
    "            p.font.size = Pt(16)\n",
    "            body_frame.word_wrap = True\n",
    "\n",
    "            # --- Add Visuals ---\n",
    "            buffer = io.BytesIO()\n",
    "            if visual_type in ('bar_chart',):\n",
    "                create_bar_chart(visual_data, buffer)\n",
    "                slide.shapes.add_picture(\n",
    "                    buffer, Inches(6.0), Inches(1.5), width=Inches(3.5))\n",
    "            elif visual_type == 'delta_chart':\n",
    "                create_delta_chart(visual_data, buffer)\n",
    "                slide.shapes.add_picture(\n",
    "                    buffer, Inches(6.0), Inches(1.5), width=Inches(3.5))\n",
    "            elif visual_type == 'gauge_meter':\n",
    "                create_gauge_meter(visual_data, buffer)\n",
    "                slide.shapes.add_picture(\n",
    "                    buffer, Inches(6.0), Inches(1.5), width=Inches(3.5))\n",
    "            elif visual_type == 'icon':\n",
    "                icon_shape = slide.shapes.add_textbox(\n",
    "                    Inches(6.5), Inches(2.0), Inches(2.0), Inches(2.0))\n",
    "                icon_frame = icon_shape.text_frame\n",
    "                icon_frame.text = visual_data.get('icon', ' ')\n",
    "                p = icon_frame.paragraphs[0]\n",
    "                p.font.size = Pt(96)\n",
    "                p.alignment = PP_ALIGN.CENTER\n",
    "            # Visual types 'comparison', 'waterfall_chart', 'timeline', 'list'\n",
    "            # are handled with text only for now.\n",
    "\n",
    "        # --- 4. Save Presentation to 'reports' Folder ---\n",
    "        reports_dir = 'reports'\n",
    "        os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "        report_filename = f\"AI_Generated_Report_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.pptx\"\n",
    "        report_path = os.path.join(reports_dir, report_filename)\n",
    "\n",
    "        prs.save(report_path)\n",
    "        print(f\"\\n✅ Success! Presentation saved to: '{report_path}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An unexpected error occurred during presentation generation: {e}\")\n",
    "\n",
    "# --- Automatically run the presentation creation ---\n",
    "create_presentation_from_dataframe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e85fe4dc312038ca",
   "metadata": {},
   "source": [
    "# Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6decf7105d13c2c",
   "metadata": {},
   "source": [
    "### LangChain Vector Store Build\n",
    "This cell disables TensorFlow/Keras for clean transformer imports, validates the required DataFrames, converts each to `Document`s, chunks them, embeds with `all-MiniLM-L6-v2`, and stores everything in a local FAISS index (`vector_store/`) exposed as `vector_store` for the chat interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ade542aced8e2322",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --- Disable TensorFlow/Keras before importing transformers -------------------\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Ensure the required packages are installed (run once per environment)\n",
    "# !pip install -q -U langchain-huggingface sentence-transformers\n",
    "\n",
    "# --- LangChain Vector DB: tokenization and indexing of DataFrames -------------\n",
    "# Prerequisite: df, results_df, df_uplift, presentation_df, project_description_df\n",
    "# must already exist in memory.\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1. Validation ----------------------------------------------------------------\n",
    "required_dfs = {\n",
    "    \"df\":                       \"Raw data (XLSX)\",\n",
    "    \"results_df\":               \"Overall results\",\n",
    "    \"df_uplift\":                \"Expected Uplift vs. Other Variants\",\n",
    "    \"presentation_df\":          \"Presentation data\",\n",
    "    \"project_description_df\":   \"Project description\"\n",
    "}\n",
    "missing = [name for name in required_dfs if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"The following DataFrames are missing in the session: {missing}\")\n",
    "\n",
    "# 2. Convert each DataFrame into a single Document -----------------------------\n",
    "docs = []\n",
    "for name, label in required_dfs.items():\n",
    "    frame = globals()[name]\n",
    "    df_as_text = frame.to_csv(index=False)\n",
    "    full_text = f\"### DataFrame: {name} – {label}\\n{df_as_text}\"\n",
    "    docs.append(\n",
    "        Document(\n",
    "            page_content=full_text,\n",
    "            metadata={\"source_df\": name, \"description\": label}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 3. Chunking ------------------------------------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 4. Embeddings ----------------------------------------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 5. Vector Store --------------------------------------------------------------\n",
    "vector_db = FAISS.from_documents(split_docs, embeddings)\n",
    "vector_store = vector_db           # <-- alias required by the chat interface\n",
    "vector_db.save_local(\"vector_store\")\n",
    "\n",
    "print(\"✅ Vector store created in 'vector_store/'.\")\n",
    "print(\"   To reload later:\")\n",
    "print(\"   from langchain.vectorstores import FAISS\")\n",
    "print(\"   vec = FAISS.load_local('vector_store', embeddings)\")\n",
    "print(\"   vector_store = vec   # make it visible to the chat function\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e112907b465dc1c",
   "metadata": {},
   "source": [
    "### Gemini Chat Model Selector\n",
    "This cell lists the Gemini models available to your API key, lets you pick one via a dropdown, and—on confirmation—stores the choice in `chat_model_name` for use by later chat cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b490c04b0526a0ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --- Select a Gemini model for chat ------------------------------------------\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Fetch compatible Gemini models in your account\n",
    "available_models = [\n",
    "    m.name for m in genai.list_models()\n",
    "    if \"generateContent\" in m.supported_generation_methods\n",
    "]\n",
    "gemini_models = [m for m in available_models if \"models/gemini\" in m]\n",
    "\n",
    "if not gemini_models:\n",
    "    raise RuntimeError(\"❌ No compatible Gemini models found in your account.\")\n",
    "\n",
    "# Dropdown: choose the model\n",
    "chat_model_selector = widgets.Dropdown(\n",
    "    options=gemini_models,\n",
    "    value=gemini_models[0],\n",
    "    description=\"Chat model:\"\n",
    ")\n",
    "\n",
    "# Confirm button\n",
    "confirm_chat_model_button = widgets.Button(\n",
    "    description=\"Use this model\",\n",
    "    button_style=\"info\"\n",
    ")\n",
    "\n",
    "# Output area for feedback\n",
    "chat_selector_output = widgets.Output()\n",
    "\n",
    "def on_confirm_chat_model_clicked(_):\n",
    "    \"\"\"Store the chosen model and show visual feedback.\"\"\"\n",
    "    with chat_selector_output:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"✅ Chat model selected: {chat_model_selector.value}\")\n",
    "    # Make the model name globally available for later cells\n",
    "    globals()[\"chat_model_name\"] = chat_model_selector.value\n",
    "\n",
    "confirm_chat_model_button.on_click(on_confirm_chat_model_clicked)\n",
    "\n",
    "# Display the UI\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [\n",
    "            widgets.HTML(\"<h3>Select a Model for the Chat</h3>\"),\n",
    "            chat_model_selector,\n",
    "            confirm_chat_model_button,\n",
    "            chat_selector_output,\n",
    "        ],\n",
    "        layout={\"border\": \"1px solid #ccc\", \"padding\": \"10px\", \"border_radius\": \"6px\"},\n",
    "    )\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b0d120fea297cfc",
   "metadata": {},
   "source": [
    "### Prompt & Basic Config Setup\n",
    "This cell defines the system prompt that disambiguates “conversion” vs. “conversion rate” and configures Gemini (model name + generation settings). It ensures later chat calls use only notebook‑scoped context and report the correct metric explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ca1ddf45fd22421",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --- 1. Prompt --------------------------------------\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "### 1. Persona e Objetivo Principal\n",
    "- Você é um analista de dados especialista, focado em um projeto de teste A/B.\n",
    "- Seu objetivo principal é responder a perguntas usando os dados fornecidos sobre o projeto.\n",
    "- Você pode e deve realizar cálculos matemáticos (somas, médias, percentuais, etc.) usando os números encontrados nos dados de contexto para responder às perguntas do usuário.\n",
    "\n",
    "### 2. Contexto e Regras Fundamentais\n",
    "- **Fonte da Verdade:** Baseie TODAS as suas respostas e cálculos estritamente nos números e informações encontradas nos chunks de texto da seção \"Notebook Context\".\n",
    "- **Regra para Informação Ausente:** Se os dados necessários para uma resposta ou cálculo não estiverem no contexto, afirme claramente qual informação está faltando. Não invente números.\n",
    "\n",
    "### 3. Lógica e Processo de Raciocínio\n",
    "- **Definições de Termos:** Siga rigorosamente estas definições:\n",
    "  - 'conversão' ou 'conversões': Refere-se ao número absoluto de conversões (geralmente da coluna 'conversion').\n",
    "  - 'taxa de conversão', 'taxa' ou 'rate': Refere-se à métrica de proporção/probabilidade (geralmente de colunas como 'posterior_mean').\n",
    "- **Regra para Lidar com Ambiguidade:** Se o usuário perguntar por \"conversão\" sem especificar \"taxa\", sua resposta deve, por padrão, ser sobre o número absoluto.\n",
    "- **Regra de Suposição para Ambiguidade Persistente:** Se uma pergunta do usuário, mesmo depois de reescrita, permanecer ambígua (ex: \"compare as variantes\"), você deve primeiro declarar a suposição que está fazendo para poder respondê-la. Não responda diretamente sem antes validar sua premissa.\n",
    "\n",
    "### 4. Formato de Saída\n",
    "- Responda sempre em português brasileiro.\n",
    "- Ao apresentar um dado numérico, sempre especifique a métrica que ele representa.\n",
    "- **Mostre seu trabalho:** Quando realizar um cálculo, explique as etapas de forma simples para que o usuário possa entender seu raciocínio. Exemplo: \"Para calcular o total, somei as conversões de todas as variantes (A+B+C+D), que resultou em X. O percentual da variante D é (587 / X) * 100, que é Y%.\"\n",
    "- **Formato da Suposição:** Ao declarar uma suposição, sua resposta deve ser APENAS a pergunta de validação. Exemplo: \"Sua pergunta é um pouco ambígua. Estou supondo que você quer uma comparação das taxas de conversão. É isso mesmo que você deseja?\"\n",
    "\"\"\"\n",
    "\n",
    "# --- Guias de Estilo para os Modos de Conversa ---\n",
    "STANDARD_STYLE_GUIDE = \"\"\"\n",
    "### Guia de Estilo de Resposta (Modo Padrão):\n",
    "- Responda de forma direta e informativa, como se estivesse em uma apresentação de resultados para uma audiência geral.\n",
    "- Foque na informação principal. NÃO mencione fontes de dados como \"Chunk 3\" ou nomes técnicos de colunas como 'posterior_mean'.\n",
    "- A linguagem deve ser natural e limpa.\n",
    "\"\"\"\n",
    "\n",
    "ANALYST_STYLE_GUIDE = \"\"\"\n",
    "### Guia de Estilo de Resposta (Modo Analista):\n",
    "- Responda como se estivesse conversando com um colega analista de dados.\n",
    "- Seja detalhista. É encorajado e esperado que você cite as fontes dos dados (ex: \"No DataFrame df_uplift...\") e os nomes das colunas/métricas (ex: 'posterior_mean', 'uplift').\n",
    "- A transparência sobre a metodologia e a origem dos dados é mais importante que a brevidade.\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_QUESTION_PROMPT = \"\"\"Dada a seguinte conversa e uma pergunta de acompanhamento, reformule a pergunta de acompanhamento para ser uma pergunta autônoma que possa ser compreendida sem o histórico da conversa.\n",
    "\n",
    "Histórico da Conversa:\n",
    "{chat_history}\n",
    "\n",
    "Pergunta de Acompanhamento:\n",
    "{question}\n",
    "\n",
    "Pergunta Autônoma:\n",
    "\"\"\"\n",
    "\n",
    "INTENT_CLASSIFICATION_PROMPT = \"\"\"Sua tarefa é classificar a intenção da mensagem do usuário em uma de duas categorias. Responda com uma única palavra: QUERY ou CHITCHAT.\n",
    "\n",
    "QUERY: O usuário está fazendo uma pergunta, pedindo dados, cálculos ou uma análise.\n",
    "CHITCHAT: O usuário está fazendo um cumprimento, agradecimento, despedida ou uma conversa casual não relacionada a dados.\n",
    "\n",
    "---\n",
    "Exemplos:\n",
    "\n",
    "Mensagem do Usuário: qual a taxa de conversão da variante C?\n",
    "Sua Classificação: QUERY\n",
    "\n",
    "Mensagem do Usuário: muito obrigado!\n",
    "Sua Classificação: CHITCHAT\n",
    "\n",
    "Mensagem do Usuário: compare os resultados\n",
    "Sua Classificação: QUERY\n",
    "\n",
    "Mensagem do Usuário: olá, tudo bem?\n",
    "Sua Classificação: CHITCHAT\n",
    "\n",
    "Mensagem do Usuário: gracias\n",
    "Sua Classificação: CHITCHAT\n",
    "---\n",
    "\n",
    "Classifique a seguinte mensagem:\n",
    "\n",
    "Mensagem do Usuário: {user_message}\n",
    "Sua Classificação:\n",
    "\"\"\"\n",
    "\n",
    "# Nome do modelo e configuração de geração\n",
    "model_name = globals().get(\"chat_model_name\", \"models/gemini-1.5-pro-latest\")\n",
    "generation_config = genai.GenerationConfig(max_output_tokens=2048)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "615b6ea93cf772bd",
   "metadata": {},
   "source": [
    "### Chatbot Functions & Interface Setup\n",
    "This cell imports the necessary libraries, instantiates the Gemini model using your previously selected configuration, and defines helper routines to:\n",
    "\n",
    "1. Retrieve the top context chunks from your FAISS vector store,\n",
    "2. Disambiguate whether the user is asking for conversion counts or rates,\n",
    "3. Build the full system + context + user prompt, and\n",
    "4. Call the LLM to generate a response.\n",
    "\n",
    "It then wires everything up into a Gradio `ChatInterface` titled “Gemini Assistant — RAG (Vector Store)” and launches it for interactive querying."
   ]
  },
  {
   "cell_type": "code",
   "id": "4bd7cb3d19f51cb6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# --- 2. Funções, chatbot e interface ---------------------------------------\n",
    "import re\n",
    "import gradio as gr\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Cria o modelo Gemini usando as configs da 1ª célula\n",
    "chat_model = genai.GenerativeModel(model_name)\n",
    "\n",
    "# Variável para controlar o modo da conversa (STANDARD ou FULL_VIEW)\n",
    "chat_mode = \"STANDARD\"\n",
    "\n",
    "# ... (funções _classify_intent, _generate_standalone_question, etc. permanecem inalteradas) ...\n",
    "def _classify_intent(user_message: str) -> str:\n",
    "    prompt = INTENT_CLASSIFICATION_PROMPT.format(user_message=user_message)\n",
    "    try:\n",
    "        response = chat_model.generate_content(prompt)\n",
    "        classification = response.text.strip().upper()\n",
    "        if classification == \"CHITCHAT\":\n",
    "            return \"CHITCHAT\"\n",
    "        else:\n",
    "            return \"QUERY\"\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao classificar intenção: {e}\")\n",
    "        return \"QUERY\"\n",
    "\n",
    "def _generate_standalone_question(question: str, history: list) -> str:\n",
    "    if not history:\n",
    "        return question\n",
    "    formatted_history = \"\"\n",
    "    for user_msg, bot_msg in history:\n",
    "        formatted_history += f\"Usuário: {user_msg}\\nAssistente: {bot_msg}\\n\"\n",
    "    prompt = REPHRASE_QUESTION_PROMPT.format(chat_history=formatted_history, question=question)\n",
    "    try:\n",
    "        response = chat_model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro ao gerar pergunta autônoma: {e}\")\n",
    "        return question\n",
    "\n",
    "def _format_chat_history(history: list, num_turns: int = 2) -> str:\n",
    "    if not history:\n",
    "        return \"\"\n",
    "    recent_history = history[-num_turns:]\n",
    "    formatted_history = \"### Histórico da Conversa Recente:\\n\"\n",
    "    for user_msg, bot_msg in recent_history:\n",
    "        formatted_history += f\"Usuário: {user_msg}\\nAssistente: {bot_msg}\\n\"\n",
    "    return formatted_history.strip()\n",
    "\n",
    "def _docs_to_context(docs: List):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"```chunk {i} | source_df={d.metadata.get('source_df')}\\n{d.page_content}\\n```\"\n",
    "        for i, d in enumerate(docs, 1)\n",
    "    ) or \"<no relevant context>\"\n",
    "\n",
    "\n",
    "def gemini_chat(message: str, history: list):\n",
    "    global chat_mode\n",
    "\n",
    "    if message.strip() == \"#full_view\":\n",
    "        chat_mode = \"FULL_VIEW\"\n",
    "        yield \"✅ Modo Analista (`full_view`) ativado. As próximas respostas incluirão fontes e detalhes técnicos.\"\n",
    "        return\n",
    "    elif message.strip() == \"#standard_mode\":\n",
    "        chat_mode = \"STANDARD\"\n",
    "        yield \"✅ Modo Padrão (`standard_mode`) ativado. As próximas respostas serão diretas e informativas.\"\n",
    "        return\n",
    "\n",
    "    intent = _classify_intent(message)\n",
    "    print(f\"--- DEBUG: Modo Atual: {chat_mode} | Intenção Detectada: {intent} ---\")\n",
    "\n",
    "    if intent == \"CHITCHAT\":\n",
    "        try:\n",
    "            chitchat_prompt = f\"Você é um assistente prestativo. O usuário disse: '{message}'. Responda de forma breve e educada.\"\n",
    "            resp = chat_model.generate_content(chitchat_prompt)\n",
    "            yield resp.text.strip()\n",
    "            return\n",
    "        except Exception as e:\n",
    "            yield f\"⚠️ Erro ao gerar resposta de chitchat: {e}\"\n",
    "            return\n",
    "    else: # A intenção é 'QUERY'\n",
    "        if \"vector_store\" not in globals():\n",
    "            yield \"❌ vector_store not found.\"\n",
    "            return\n",
    "\n",
    "        standalone_question = _generate_standalone_question(message, history)\n",
    "        print(f\"--- DEBUG: Pergunta Reescreita: '{standalone_question}' ---\")\n",
    "\n",
    "        try:\n",
    "            docs_scores = vector_store.similarity_search_with_score(standalone_question, k=8)\n",
    "            docs = [d for d, _ in docs_scores]\n",
    "        except Exception:\n",
    "            docs = vector_store.similarity_search(standalone_question, k=8)\n",
    "        except Exception as e:\n",
    "            yield f\"<error querying vector_store: {e}>\"\n",
    "            return\n",
    "\n",
    "        chat_history_for_prompt = _format_chat_history(history)\n",
    "        notebook_context = _docs_to_context(docs)\n",
    "        metric_hint = \"auto\"\n",
    "\n",
    "        # --- INÍCIO DA NOVA LÓGICA DESTA ETAPA ---\n",
    "        # Seleciona o guia de estilo com base no modo atual\n",
    "        if chat_mode == \"FULL_VIEW\":\n",
    "            style_guide = ANALYST_STYLE_GUIDE\n",
    "        else: # O padrão é STANDARD\n",
    "            style_guide = STANDARD_STYLE_GUIDE\n",
    "\n",
    "        # Monta o prompt final dinamicamente\n",
    "        prompt = (\n",
    "            f\"{SYSTEM_PROMPT}\\n\\n\"\n",
    "            f\"{style_guide}\\n\\n\"  # <-- Guia de estilo dinâmico inserido aqui\n",
    "            f\"{chat_history_for_prompt}\\n\\n\"\n",
    "            f\"### Documentos de Contexto (Resultados da Busca):\\n{notebook_context}\\n\\n\"\n",
    "            f\"Metric hint: {metric_hint}\\n\\n\"\n",
    "            f\"### Pergunta a ser Respondida:\\n{standalone_question}\"\n",
    "        )\n",
    "        # --- FIM DA NOVA LÓGICA ---\n",
    "\n",
    "        try:\n",
    "            resp = chat_model.generate_content(prompt, generation_config=generation_config, stream=False)\n",
    "            if resp.candidates and resp.candidates[0].content.parts:\n",
    "                response_text = resp.candidates[0].content.parts[0].text.strip()\n",
    "                if \"É isso mesmo que você deseja?\" in response_text:\n",
    "                    yield response_text\n",
    "                else:\n",
    "                    yield response_text\n",
    "            else:\n",
    "                fr_code = \"N/A\"\n",
    "                if resp.candidates:\n",
    "                    fr_code = resp.candidates[0].finish_reason.name\n",
    "                yield f\"⚠️ O modelo não produziu texto (finish_reason={fr_code}).\"\n",
    "        except Exception as e:\n",
    "            yield f\"⚠️ Erro ao chamar o Gemini: {e}\"\n",
    "\n",
    "# --- 3. Bloco de Interface (CÓDIGO FALTANTE) -------------------------------\n",
    "# Adicione este bloco para criar e executar a interface conversacional\n",
    "\n",
    "chat_ui = gr.ChatInterface(\n",
    "    fn=gemini_chat,\n",
    "    title=\"Gemini Assistant com Modos de Análise\",\n",
    "    description=\"Faça perguntas sobre o contexto. Use #full_view ou #standard_mode para alterar o estilo da resposta.\",\n",
    "    examples=[\n",
    "        [\"Qual foi a taxa de conversão do último trimestre?\"],\n",
    "        [\"#full_view\"],\n",
    "        [\"#standard_mode\"]\n",
    "    ],\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    retry_btn=None,\n",
    "    undo_btn=\"Desfazer\",\n",
    "    clear_btn=\"Limpar\"\n",
    ").queue()\n",
    "\n",
    "# Para iniciar a interface no notebook ou script\n",
    "# O debug=True é útil para ver os logs (como os prints) no terminal.\n",
    "chat_ui.launch(debug=True, share=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
